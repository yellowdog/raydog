# Confiuration for the YellowDog RayDog autoscaler
provider:
  type: external
  module: raydog.autoscaler.RayDogNodeProvider
  namespace: raydog-example
  lifetime: 30m
  build_timeout: 10m

  initialization_script: file:example-node-setup.sh

  head_start_ray_script: 
  - "#!/bin/bash -x"
  - export PYTHONPATH=/opt/yellowdog/agent:$PYTHONPATH
  - trap "ray stop; echo Ray stopped" EXIT
  #- set -euo pipefail
  - VENV=/opt/yellowdog/agent/venv
  - source $VENV/bin/activate
  - ray stop
  - ray start --disable-usage-stats --head --port=6379 --autoscaling-config=/opt/yellowdog/agent/raydog-autoscaler.yaml --block

  worker_start_ray_script:
  - "#!/bin/bash"
  - export "PYTHONPATH=/opt/yellowdog/agent:$PYTHONPATH"
  - trap "ray stop; echo Ray stopped" EXIT
  - set -euo pipefail
  - VENV=/opt/yellowdog/agent/venv
  - source $VENV/bin/activate
  - ray stop
  - ray start --disable-usage-stats --address=$RAY_HEAD_IP:6379 --block

# A unique identifier for the head node and workers of this cluster. Not case sensitive
cluster_name: raydog-123ABCxyz

# The types of nodes that are available
available_node_types:
  ray_head_default:
    resources: { "CPU": 0 }
    node_config:
      compute_requirement_template: yd-demo/yd-demo-aws-eu-west-1-split-ondemand           
      images_id: ydid:imgfam:48592E:f18416ef-fb11-493d-add6-8d834d58289c
      max_nodes: 1

  ray_worker_small:
    resources: { "CPU": 2 }
    min_workers: 1
    max_workers: 2    
    node_config:
      compute_requirement_template: yd-demo/yd-demo-aws-eu-west-1-split-ondemand           
      images_id: ydid:imgfam:48592E:f18416ef-fb11-493d-add6-8d834d58289c
      max_nodes: 100
      #images_id: ami-0fef583e486727263

# Specify the node type of the head node (as configured above).
head_node_type: ray_head_default

# How Ray will authenticate with newly launched nodes.
auth:
  ssh_user: yd-agent
  ssh_private_key: private-key

# The maximum number of workers nodes to launch in addition to the head
# node.
max_workers: 2

# The autoscaler will scale up the cluster faster with higher upscaling speed.
# E.g., if the task requires adding more nodes then autoscaler will gradually
# scale up the cluster in chunks of upscaling_speed*currently_running_nodes.
# This number should be > 0.
upscaling_speed: 1.0

# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 5

# Files or directories to copy to the head and worker nodes. The format is a
# dictionary from REMOTE_PATH: LOCAL_PATH, e.g.
file_mounts: {
#    "/path1/on/remote/machine": "/path1/on/local/machine",
#    "/path2/on/remote/machine": "/path2/on/local/machine",
}

# Files or directories to copy from the head node to the worker nodes. The format is a
# list of paths. The same path on the head node will be copied to the worker node.
# This behavior is a subset of the file_mounts behavior. In the vast majority of cases
# you should just use file_mounts. Only use this if you know what you're doing!
cluster_synced_files: []

# Whether changes to directories in file_mounts or cluster_synced_files in the head node
# should sync to the worker node continuously
file_mounts_sync_continuously: False

# Patterns for files to exclude when running rsync up or rsync down
rsync_exclude:
    - "**/.git"
    - "**/.git/**"

# Pattern files to use for filtering out files when running rsync up or rsync down. The file is searched for
# in the source directory and recursively through all subdirectories. For example, if .gitignore is provided
# as a value, the behavior will match git's behavior for finding and using .gitignore files.
rsync_filter:
    - ".gitignore"

# RayDog handles setting-up nodes up and starting Ray 
# Do any customisation you need in the 'provider' section
initialization_commands: []

setup_commands: []

head_setup_commands: []

worker_setup_commands: []

head_start_ray_commands: []

worker_start_ray_commands: []
