# Configuration for the YellowDog RayDog autoscaler
provider:
  type: external
  module: raydog.autoscaler.RayDogNodeProvider
  
  namespace: raydog-example
  lifetime: 60m
  build_timeout: 10m

  foreground_node_launch: True

  initialization_script: file:example-node-setup.sh

  head_start_ray_script: 
  - "#!/bin/bash -x"
  - "source $HOME/.bashrc"
  - "$HOME/valkey*/bin/valkey-server --port 16667 &"
  - "until [ -f $HOME/ray_bootstrap_key.pem ]; do sleep 2; done"
  - trap "ray stop; echo Ray stopped" EXIT
  - set -euo pipefail
  - ray stop
  - ray --logging-level debug  start --disable-usage-stats --head --port=6379 --autoscaling-config=/opt/yellowdog/agent/ray_bootstrap_config.yaml --block
 
  worker_start_ray_script:
  - "#!/bin/bash"
  - "source $HOME/.bashrc"
  - trap "ray stop; echo Ray stopped" EXIT
  - set -euo pipefail
  - ray stop
  - ray start --disable-usage-stats --address=$RAY_HEAD_IP:6379 --block

  # Have to duplicate the auth setttings, so that the RayDog provider can access the tag store
  auth:
    ssh_user: yd-agent
    ssh_private_key: ../private-key

# A unique identifier for the head node and workers of this cluster. Not case sensitive
cluster_name: raydog-123ABCxyz

# The types of nodes that are available
available_node_types:
  ray_head_default:
    resources: { "CPU": 0 }
    node_config:
      compute_requirement_template: yd-demo/yd-demo-aws-eu-west-2-split-ondemand-rayhead
      images_id: ami-0c6175878f7e01e70
      max_nodes: 1

  ray_worker_small:
    resources: { "CPU": 1 }
    min_workers: 1
    max_workers: 100    
    node_config:
      compute_requirement_template: yd-demo/yd-demo-aws-eu-west-2-split-ondemand-rayworker
      images_id: ami-0c6175878f7e01e70
      max_nodes: 100

# Specify the node type of the head node (as configured above).
head_node_type: ray_head_default

# How Ray will authenticate with newly launched nodes.
auth:
  ssh_user: yd-agent
  ssh_private_key: ../private-key

# The maximum number of workers nodes to launch in addition to the head
# node.
max_workers: 1000

# The autoscaler will scale up the cluster faster with higher upscaling speed.
# E.g., if the task requires adding more nodes then autoscaler will gradually
# scale up the cluster in chunks of upscaling_speed*currently_running_nodes.
# This number should be > 0.
upscaling_speed: 1.0

# If a node is idle for this many minutes, it will be removed.
idle_timeout_minutes: 2

# Files or directories to copy to the head and worker nodes. The format is a
# dictionary from REMOTE_PATH: LOCAL_PATH, e.g.
file_mounts: {
#    "/path1/on/remote/machine": "/path1/on/local/machine",
#    "/path2/on/remote/machine": "/path2/on/local/machine",
}

# Files or directories to copy from the head node to the worker nodes. The format is a
# list of paths. The same path on the head node will be copied to the worker node.
# This behavior is a subset of the file_mounts behavior. In the vast majority of cases
# you should just use file_mounts. Only use this if you know what you're doing!
cluster_synced_files: []

# Whether changes to directories in file_mounts or cluster_synced_files in the head node
# should sync to the worker node continuously
file_mounts_sync_continuously: False

# Patterns for files to exclude when running rsync up or rsync down
rsync_exclude:
    - "**/.git"
    - "**/.git/**"

# Pattern files to use for filtering out files when running rsync up or rsync down. The file is searched for
# in the source directory and recursively through all subdirectories. For example, if .gitignore is provided
# as a value, the behavior will match git's behavior for finding and using .gitignore files.
rsync_filter:
    - ".gitignore"

# RayDog handles setting-up nodes up and starting Ray 
# Do any customisation you need in the 'provider' section
initialization_commands: []

setup_commands: []

head_setup_commands: []

worker_setup_commands: []

head_start_ray_commands: []

worker_start_ray_commands: []
